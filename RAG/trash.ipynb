{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-openai in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-embeddings-openai) (0.11.14)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-embeddings-openai) (1.50.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.10.6)\n",
      "Requirement already satisfied: dataclasses-json in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.9.0)\n",
      "Requirement already satisfied: httpx in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.12.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.22.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Using cached llama_index-0.11.14-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
      "  Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
      "  Using cached llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.14 (from llama-index)\n",
      "  Using cached llama_index_core-0.11.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Using cached llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.9 (from llama-index)\n",
      "  Using cached llama_index_llms_openai-0.2.9-py3-none-any.whl.metadata (648 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Using cached openai-1.50.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached SQLAlchemy-2.0.35-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached aiohttp-3.10.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting nest-asyncio<2.0.0,>=1.5.8 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting numpy<2.0.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
      "  Using cached llama_cloud-0.1.0-py3-none-any.whl.metadata (750 bytes)\n",
      "Collecting pandas (from llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
      "  Using cached llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting click (from nltk>3.8.1->llama-index)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached aiohappyeyeballs-2.4.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached yarl-1.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (49 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting packaging>=17.0 (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.14->llama-index)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached llama_index-0.11.14-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Using cached llama_index_core-0.11.14-py3-none-any.whl (1.6 MB)\n",
      "Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
      "Using cached llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "Using cached llama_index_llms_openai-0.2.9-py3-none-any.whl (12 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached aiohttp-3.10.6-cp311-cp311-macosx_11_0_arm64.whl (390 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached llama_cloud-0.1.0-py3-none-any.whl (176 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached llama_parse-0.5.6-py3-none-any.whl (10 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached openai-1.50.0-py3-none-any.whl (378 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached SQLAlchemy-2.0.35-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached aiohappyeyeballs-2.4.1-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl (299 kB)\n",
      "Using cached marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached yarl-1.12.1-cp311-cp311-macosx_11_0_arm64.whl (112 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: striprtf, pytz, dirtyjson, wrapt, urllib3, tzdata, typing-extensions, tqdm, tenacity, soupsieve, sniffio, six, regex, PyYAML, pypdf, pillow, packaging, numpy, networkx, nest-asyncio, mypy-extensions, multidict, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, python-dateutil, pydantic-core, nltk, marshmallow, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, pydantic, pandas, httpx, dataclasses-json, aiohttp, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: striprtf\n",
      "    Found existing installation: striprtf 0.0.26\n",
      "    Uninstalling striprtf-0.0.26:\n",
      "      Successfully uninstalled striprtf-0.0.26\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.2\n",
      "    Uninstalling pytz-2024.2:\n",
      "      Successfully uninstalled pytz-2024.2\n",
      "  Attempting uninstall: dirtyjson\n",
      "    Found existing installation: dirtyjson 1.0.8\n",
      "    Uninstalling dirtyjson-1.0.8:\n",
      "      Successfully uninstalled dirtyjson-1.0.8\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.2\n",
      "    Uninstalling tzdata-2024.2:\n",
      "      Successfully uninstalled tzdata-2024.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.5.0\n",
      "    Uninstalling tenacity-8.5.0:\n",
      "      Successfully uninstalled tenacity-8.5.0\n",
      "  Attempting uninstall: soupsieve\n",
      "    Found existing installation: soupsieve 2.6\n",
      "    Uninstalling soupsieve-2.6:\n",
      "      Successfully uninstalled soupsieve-2.6\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.9.11\n",
      "    Uninstalling regex-2024.9.11:\n",
      "      Successfully uninstalled regex-2024.9.11\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: pypdf\n",
      "    Found existing installation: pypdf 4.3.1\n",
      "    Uninstalling pypdf-4.3.1:\n",
      "      Successfully uninstalled pypdf-4.3.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.3\n",
      "    Uninstalling networkx-3.3:\n",
      "      Successfully uninstalled networkx-3.3\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.6.0\n",
      "    Uninstalling nest-asyncio-1.6.0:\n",
      "      Successfully uninstalled nest-asyncio-1.6.0\n",
      "  Attempting uninstall: mypy-extensions\n",
      "    Found existing installation: mypy-extensions 1.0.0\n",
      "    Uninstalling mypy-extensions-1.0.0:\n",
      "      Successfully uninstalled mypy-extensions-1.0.0\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.1.0\n",
      "    Uninstalling multidict-6.1.0:\n",
      "      Successfully uninstalled multidict-6.1.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: jiter\n",
      "    Found existing installation: jiter 0.5.0\n",
      "    Uninstalling jiter-0.5.0:\n",
      "      Successfully uninstalled jiter-0.5.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 3.1.1\n",
      "    Uninstalling greenlet-3.1.1:\n",
      "      Successfully uninstalled greenlet-3.1.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.1\n",
      "    Uninstalling frozenlist-1.4.1:\n",
      "      Successfully uninstalled frozenlist-1.4.1\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.9.0\n",
      "    Uninstalling distro-1.9.0:\n",
      "      Successfully uninstalled distro-1.9.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "    Found existing installation: aiohappyeyeballs 2.4.1\n",
      "    Uninstalling aiohappyeyeballs-2.4.1:\n",
      "      Successfully uninstalled aiohappyeyeballs-2.4.1\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.12.1\n",
      "    Uninstalling yarl-1.12.1:\n",
      "      Successfully uninstalled yarl-1.12.1\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.9.0\n",
      "    Uninstalling typing-inspect-0.9.0:\n",
      "      Successfully uninstalled typing-inspect-0.9.0\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.35\n",
      "    Uninstalling SQLAlchemy-2.0.35:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.35\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.23.4\n",
      "    Uninstalling pydantic_core-2.23.4:\n",
      "      Successfully uninstalled pydantic_core-2.23.4\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 3.22.0\n",
      "    Uninstalling marshmallow-3.22.0:\n",
      "      Successfully uninstalled marshmallow-3.22.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.5\n",
      "    Uninstalling httpcore-1.0.5:\n",
      "      Successfully uninstalled httpcore-1.0.5\n",
      "  Attempting uninstall: deprecated\n",
      "    Found existing installation: Deprecated 1.2.14\n",
      "    Uninstalling Deprecated-1.2.14:\n",
      "      Successfully uninstalled Deprecated-1.2.14\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.3\n",
      "    Uninstalling beautifulsoup4-4.12.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.3\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.6.0\n",
      "    Uninstalling anyio-4.6.0:\n",
      "      Successfully uninstalled anyio-4.6.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.7.0\n",
      "    Uninstalling tiktoken-0.7.0:\n",
      "      Successfully uninstalled tiktoken-0.7.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.9.2\n",
      "    Uninstalling pydantic-2.9.2:\n",
      "      Successfully uninstalled pydantic-2.9.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.2\n",
      "    Uninstalling httpx-0.27.2:\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.6.7\n",
      "    Uninstalling dataclasses-json-0.6.7:\n",
      "      Successfully uninstalled dataclasses-json-0.6.7\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.10.6\n",
      "    Uninstalling aiohttp-3.10.6:\n",
      "      Successfully uninstalled aiohttp-3.10.6\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.50.0\n",
      "    Uninstalling openai-1.50.0:\n",
      "      Successfully uninstalled openai-1.50.0\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.11.14\n",
      "    Uninstalling llama-index-core-0.11.14:\n",
      "      Successfully uninstalled llama-index-core-0.11.14\n",
      "  Attempting uninstall: llama-cloud\n",
      "    Found existing installation: llama-cloud 0.1.0\n",
      "    Uninstalling llama-cloud-0.1.0:\n",
      "      Successfully uninstalled llama-cloud-0.1.0\n",
      "  Attempting uninstall: llama-parse\n",
      "    Found existing installation: llama-parse 0.5.6\n",
      "    Uninstalling llama-parse-0.5.6:\n",
      "      Successfully uninstalled llama-parse-0.5.6\n",
      "  Attempting uninstall: llama-index-readers-file\n",
      "    Found existing installation: llama-index-readers-file 0.2.2\n",
      "    Uninstalling llama-index-readers-file-0.2.2:\n",
      "      Successfully uninstalled llama-index-readers-file-0.2.2\n",
      "  Attempting uninstall: llama-index-llms-openai\n",
      "    Found existing installation: llama-index-llms-openai 0.2.9\n",
      "    Uninstalling llama-index-llms-openai-0.2.9:\n",
      "      Successfully uninstalled llama-index-llms-openai-0.2.9\n",
      "  Attempting uninstall: llama-index-legacy\n",
      "    Found existing installation: llama-index-legacy 0.9.48.post3\n",
      "    Uninstalling llama-index-legacy-0.9.48.post3:\n",
      "      Successfully uninstalled llama-index-legacy-0.9.48.post3\n",
      "  Attempting uninstall: llama-index-indices-managed-llama-cloud\n",
      "    Found existing installation: llama-index-indices-managed-llama-cloud 0.4.0\n",
      "    Uninstalling llama-index-indices-managed-llama-cloud-0.4.0:\n",
      "      Successfully uninstalled llama-index-indices-managed-llama-cloud-0.4.0\n",
      "  Attempting uninstall: llama-index-embeddings-openai\n",
      "    Found existing installation: llama-index-embeddings-openai 0.2.5\n",
      "    Uninstalling llama-index-embeddings-openai-0.2.5:\n",
      "      Successfully uninstalled llama-index-embeddings-openai-0.2.5\n",
      "  Attempting uninstall: llama-index-readers-llama-parse\n",
      "    Found existing installation: llama-index-readers-llama-parse 0.3.0\n",
      "    Uninstalling llama-index-readers-llama-parse-0.3.0:\n",
      "      Successfully uninstalled llama-index-readers-llama-parse-0.3.0\n",
      "  Attempting uninstall: llama-index-multi-modal-llms-openai\n",
      "    Found existing installation: llama-index-multi-modal-llms-openai 0.2.1\n",
      "    Uninstalling llama-index-multi-modal-llms-openai-0.2.1:\n",
      "      Successfully uninstalled llama-index-multi-modal-llms-openai-0.2.1\n",
      "  Attempting uninstall: llama-index-cli\n",
      "    Found existing installation: llama-index-cli 0.3.1\n",
      "    Uninstalling llama-index-cli-0.3.1:\n",
      "      Successfully uninstalled llama-index-cli-0.3.1\n",
      "  Attempting uninstall: llama-index-agent-openai\n",
      "    Found existing installation: llama-index-agent-openai 0.3.4\n",
      "    Uninstalling llama-index-agent-openai-0.3.4:\n",
      "      Successfully uninstalled llama-index-agent-openai-0.3.4\n",
      "  Attempting uninstall: llama-index-program-openai\n",
      "    Found existing installation: llama-index-program-openai 0.2.0\n",
      "    Uninstalling llama-index-program-openai-0.2.0:\n",
      "      Successfully uninstalled llama-index-program-openai-0.2.0\n",
      "  Attempting uninstall: llama-index-question-gen-openai\n",
      "    Found existing installation: llama-index-question-gen-openai 0.2.0\n",
      "    Uninstalling llama-index-question-gen-openai-0.2.0:\n",
      "      Successfully uninstalled llama-index-question-gen-openai-0.2.0\n",
      "  Attempting uninstall: llama-index\n",
      "    Found existing installation: llama-index 0.11.14\n",
      "    Uninstalling llama-index-0.11.14:\n",
      "      Successfully uninstalled llama-index-0.11.14\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "rectools 0.8.0 requires attrs<24.0.0,>=19.1.0, but you have attrs 24.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.1 aiohttp-3.10.6 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.0 attrs-24.2.0 beautifulsoup4-4.12.3 certifi-2024.8.30 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.4.1 fsspec-2024.9.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 idna-3.10 jiter-0.5.0 joblib-1.4.2 llama-cloud-0.1.0 llama-index-0.11.14 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.14 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.9 llama-index-multi-modal-llms-openai-0.2.1 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.6 marshmallow-3.22.0 multidict-6.1.0 mypy-extensions-1.0.0 nest-asyncio-1.6.0 networkx-3.3 nltk-3.9.1 numpy-1.26.4 openai-1.50.0 packaging-24.1 pandas-2.2.3 pillow-10.4.0 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-4.3.1 python-dateutil-2.9.0.post0 pytz-2024.2 regex-2024.9.11 requests-2.32.3 six-1.16.0 sniffio-1.3.1 soupsieve-2.6 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.7.0 tqdm-4.66.5 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 wrapt-1.16.0 yarl-1.12.1\n",
      "Collecting llama-index-embeddings-openai\n",
      "  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.0 (from llama-index-embeddings-openai)\n",
      "  Using cached llama_index_core-0.11.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting openai>=1.1.0 (from llama-index-embeddings-openai)\n",
      "  Using cached openai-1.50.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached SQLAlchemy-2.0.35-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached aiohttp-3.10.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting nest-asyncio<2.0.0,>=1.5.8 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy<2.0.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=1.1.0->llama-index-embeddings-openai)\n",
      "  Using cached anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-embeddings-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.1.0->llama-index-embeddings-openai)\n",
      "  Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting sniffio (from openai>=1.1.0->llama-index-embeddings-openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached aiohappyeyeballs-2.4.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached yarl-1.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (49 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting packaging>=17.0 (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached llama_index_core-0.11.14-py3-none-any.whl (1.6 MB)\n",
      "Using cached openai-1.50.0-py3-none-any.whl (378 kB)\n",
      "Using cached aiohttp-3.10.6-cp311-cp311-macosx_11_0_arm64.whl (390 kB)\n",
      "Using cached anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl (299 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached SQLAlchemy-2.0.35-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.4.1-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached yarl-1.12.1-cp311-cp311-macosx_11_0_arm64.whl (112 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: dirtyjson, wrapt, urllib3, typing-extensions, tqdm, tenacity, sniffio, regex, PyYAML, pillow, packaging, numpy, networkx, nest-asyncio, mypy-extensions, multidict, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, nltk, marshmallow, httpcore, deprecated, anyio, aiosignal, tiktoken, pydantic, httpx, dataclasses-json, aiohttp, openai, llama-index-core, llama-index-embeddings-openai\n",
      "  Attempting uninstall: dirtyjson\n",
      "    Found existing installation: dirtyjson 1.0.8\n",
      "    Uninstalling dirtyjson-1.0.8:\n",
      "      Successfully uninstalled dirtyjson-1.0.8\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.5.0\n",
      "    Uninstalling tenacity-8.5.0:\n",
      "      Successfully uninstalled tenacity-8.5.0\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.9.11\n",
      "    Uninstalling regex-2024.9.11:\n",
      "      Successfully uninstalled regex-2024.9.11\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.3\n",
      "    Uninstalling networkx-3.3:\n",
      "      Successfully uninstalled networkx-3.3\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.6.0\n",
      "    Uninstalling nest-asyncio-1.6.0:\n",
      "      Successfully uninstalled nest-asyncio-1.6.0\n",
      "  Attempting uninstall: mypy-extensions\n",
      "    Found existing installation: mypy-extensions 1.0.0\n",
      "    Uninstalling mypy-extensions-1.0.0:\n",
      "      Successfully uninstalled mypy-extensions-1.0.0\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.1.0\n",
      "    Uninstalling multidict-6.1.0:\n",
      "      Successfully uninstalled multidict-6.1.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: jiter\n",
      "    Found existing installation: jiter 0.5.0\n",
      "    Uninstalling jiter-0.5.0:\n",
      "      Successfully uninstalled jiter-0.5.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 3.1.1\n",
      "    Uninstalling greenlet-3.1.1:\n",
      "      Successfully uninstalled greenlet-3.1.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.1\n",
      "    Uninstalling frozenlist-1.4.1:\n",
      "      Successfully uninstalled frozenlist-1.4.1\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.9.0\n",
      "    Uninstalling distro-1.9.0:\n",
      "      Successfully uninstalled distro-1.9.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 24.2.0\n",
      "    Uninstalling attrs-24.2.0:\n",
      "      Successfully uninstalled attrs-24.2.0\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "    Found existing installation: aiohappyeyeballs 2.4.1\n",
      "    Uninstalling aiohappyeyeballs-2.4.1:\n",
      "      Successfully uninstalled aiohappyeyeballs-2.4.1\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.12.1\n",
      "    Uninstalling yarl-1.12.1:\n",
      "      Successfully uninstalled yarl-1.12.1\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.9.0\n",
      "    Uninstalling typing-inspect-0.9.0:\n",
      "      Successfully uninstalled typing-inspect-0.9.0\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.35\n",
      "    Uninstalling SQLAlchemy-2.0.35:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.35\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.23.4\n",
      "    Uninstalling pydantic_core-2.23.4:\n",
      "      Successfully uninstalled pydantic_core-2.23.4\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 3.22.0\n",
      "    Uninstalling marshmallow-3.22.0:\n",
      "      Successfully uninstalled marshmallow-3.22.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.5\n",
      "    Uninstalling httpcore-1.0.5:\n",
      "      Successfully uninstalled httpcore-1.0.5\n",
      "  Attempting uninstall: deprecated\n",
      "    Found existing installation: Deprecated 1.2.14\n",
      "    Uninstalling Deprecated-1.2.14:\n",
      "      Successfully uninstalled Deprecated-1.2.14\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.6.0\n",
      "    Uninstalling anyio-4.6.0:\n",
      "      Successfully uninstalled anyio-4.6.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.7.0\n",
      "    Uninstalling tiktoken-0.7.0:\n",
      "      Successfully uninstalled tiktoken-0.7.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.9.2\n",
      "    Uninstalling pydantic-2.9.2:\n",
      "      Successfully uninstalled pydantic-2.9.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.2\n",
      "    Uninstalling httpx-0.27.2:\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.6.7\n",
      "    Uninstalling dataclasses-json-0.6.7:\n",
      "      Successfully uninstalled dataclasses-json-0.6.7\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.10.6\n",
      "    Uninstalling aiohttp-3.10.6:\n",
      "      Successfully uninstalled aiohttp-3.10.6\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.50.0\n",
      "    Uninstalling openai-1.50.0:\n",
      "      Successfully uninstalled openai-1.50.0\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.11.14\n",
      "    Uninstalling llama-index-core-0.11.14:\n",
      "      Successfully uninstalled llama-index-core-0.11.14\n",
      "  Attempting uninstall: llama-index-embeddings-openai\n",
      "    Found existing installation: llama-index-embeddings-openai 0.2.5\n",
      "    Uninstalling llama-index-embeddings-openai-0.2.5:\n",
      "      Successfully uninstalled llama-index-embeddings-openai-0.2.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "rectools 0.8.0 requires attrs<24.0.0,>=19.1.0, but you have attrs 24.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.1 aiohttp-3.10.6 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.0 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.4.1 fsspec-2024.9.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 idna-3.10 jiter-0.5.0 joblib-1.4.2 llama-index-core-0.11.14 llama-index-embeddings-openai-0.2.5 marshmallow-3.22.0 multidict-6.1.0 mypy-extensions-1.0.0 nest-asyncio-1.6.0 networkx-3.3 nltk-3.9.1 numpy-1.26.4 openai-1.50.0 packaging-24.1 pillow-10.4.0 pydantic-2.9.2 pydantic-core-2.23.4 regex-2024.9.11 requests-2.32.3 sniffio-1.3.1 tenacity-8.5.0 tiktoken-0.7.0 tqdm-4.66.5 typing-extensions-4.12.2 typing-inspect-0.9.0 urllib3-2.2.3 wrapt-1.16.0 yarl-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall llama-index\n",
    "!pip install --force-reinstall llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juan/Documents/proyectos_propios/jobs/teamstation/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# global\n",
    "#Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
    "Settings.embed_model = embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# per-index\n",
    "#embed_model = OpenAIEmbedding()\n",
    "#index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Nodes in the Index:\n",
      "Node ID: 7aded2b4-d018-4d33-af83-dd63a4d339a9, Embedding: None\n",
      "Node ID: 34903fec-9b73-4362-95f1-756171c47899, Embedding: None\n",
      "Node ID: a6efa19d-ab07-474d-9150-63486cc0fd4b, Embedding: None\n",
      "Node ID: eede9b3c-0d15-456c-8a1a-30954d2c472f, Embedding: None\n",
      "Node ID: 39201157-b86f-4e2f-ad23-9a1935ce1149, Embedding: None\n",
      "Node ID: 1365073a-f160-46eb-a1da-8c4a88ab727f, Embedding: None\n",
      "Node ID: 5ceb08e8-287a-4af7-981c-8b7590dfcba6, Embedding: None\n",
      "Node ID: 36d47d1e-93ef-45b2-9fc1-b37b94095d08, Embedding: None\n",
      "Node ID: ad8c3aa7-6a85-4222-9571-f2dc46b11b05, Embedding: None\n",
      "Node ID: cb2f270b-f0dc-48bd-8d73-4c58142f4e59, Embedding: None\n",
      "Node ID: e73cc5f7-b815-418c-a4c7-b6ffabebb55f, Embedding: None\n",
      "Node ID: af4e7df9-545f-4418-b643-325a4d23a1a6, Embedding: None\n",
      "Node ID: 464acd52-1dc2-446f-8574-0899494b091f, Embedding: None\n",
      "Node ID: befc0591-5a59-4cf3-9c06-8d5f509943f1, Embedding: None\n",
      "Node ID: 8257264d-421c-48f9-8429-a87ca2de756d, Embedding: None\n",
      "Node ID: 7d7f683f-00fd-4507-8117-7c310954be75, Embedding: None\n",
      "Node ID: 28667769-dafb-41c1-bf87-af3c561b5298, Embedding: None\n",
      "Node ID: 15cf69ef-ff4d-4d0c-89ae-d92d3caef490, Embedding: None\n",
      "Node ID: 307cb853-424c-4a8d-8da1-caae2a3574a7, Embedding: None\n",
      "Node ID: 29af4a01-efc8-46ed-a0f5-6c7d0d4bcc43, Embedding: None\n",
      "Node ID: 4c7065ab-2ed6-424a-b8ac-0fc77cf62f7a, Embedding: None\n",
      "Node ID: e4354355-2aca-4f96-b109-00cff492e673, Embedding: None\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all nodes from the index\n",
    "all_nodes = index.docstore.docs.values()\n",
    "\n",
    "# Print out all node IDs and their embeddings\n",
    "print(\"All Nodes in the Index:\")\n",
    "for node in all_nodes:\n",
    "    print(f\"Node ID: {node.id_}, Embedding: {node.embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"The individual mentioned in the context information initially considered starting a company to put art galleries online but later shifted focus to building online stores after observing the emergence of online stores resembling the sites they were generating for galleries. This shift in focus led to the creation of a new company called Viaweb, which received seed funding and eventually became the model for Y Combinator's investment structure.\", source_nodes=[NodeWithScore(node=TextNode(id_='4c7065ab-2ed6-424a-b8ac-0fc77cf62f7a', embedding=None, metadata={'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e78b14b2-3965-422c-8243-88726ffe0d08', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}, hash='75d96bd23dc5a771b221b0dd27e6c5d67b028f2bc39ae0e8cf53e0ded1f10145'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='29af4a01-efc8-46ed-a0f5-6c7d0d4bcc43', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}, hash='4258c8ff4c0e0b1c2295da03d5c2dc6e25fb778dda50e06de646cf0ae74f3ac8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e4354355-2aca-4f96-b109-00cff492e673', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='337456b71908d9614d811e11d697981cf0974a9e7f07219fa6fd4bd933414df6')}, text='In the art world, money and coolness are tightly coupled. Anything expensive comes to be seen as cool, and anything seen as cool will soon become equally expensive.\\n\\n[7] Technically the apartment wasn\\'t rent-controlled but rent-stabilized, but this is a refinement only New Yorkers would know or care about. The point is that it was really cheap, less than half market price.\\n\\n[8] Most software you can launch as soon as it\\'s done. But when the software is an online store builder and you\\'re hosting the stores, if you don\\'t have any users yet, that fact will be painfully obvious. So before we could launch publicly we had to launch privately, in the sense of recruiting an initial set of users and making sure they had decent-looking stores.\\n\\n[9] We\\'d had a code editor in Viaweb for users to define their own page styles. They didn\\'t know it, but they were editing Lisp expressions underneath. But this wasn\\'t an app editor, because the code ran when the merchants\\' sites were generated, not when shoppers visited them.\\n\\n[10] This was the first instance of what is now a familiar experience, and so was what happened next, when I read the comments and found they were full of angry people. How could I claim that Lisp was better than other languages? Weren\\'t they all Turing complete? People who see the responses to essays I write sometimes tell me how sorry they feel for me, but I\\'m not exaggerating when I reply that it has always been like this, since the very beginning. It comes with the territory. An essay must tell readers things they don\\'t already know, and some people dislike being told such things.\\n\\n[11] People put plenty of stuff on the internet in the 90s of course, but putting something online is not the same as publishing it online. Publishing online means you treat the online version as the (or at least a) primary version.\\n\\n[12] There is a general lesson here that our experience with Y Combinator also teaches: Customs continue to constrain you long after the restrictions that caused them have disappeared. Customary VC practice had once, like the customs about publishing essays, been based on real constraints. Startups had once been much more expensive to start, and proportionally rare. Now they could be cheap and common, but the VCs\\' customs still reflected the old world, just as customs about writing essays still reflected the constraints of the print era.\\n\\nWhich in turn implies that people who are independent-minded (i.e. less influenced by custom) will have an advantage in fields affected by rapid change (where customs are more likely to be obsolete).\\n\\nHere\\'s an interesting point, though: you can\\'t always predict which fields will be affected by rapid change. Obviously software and venture capital will be, but who would have predicted that essay writing would be?\\n\\n[13] Y Combinator was not the original name. At first we were called Cambridge Seed. But we didn\\'t want a regional name, in case someone copied us in Silicon Valley, so we renamed ourselves after one of the coolest tricks in the lambda calculus, the Y combinator.\\n\\nI picked orange as our color partly because it\\'s the warmest, and partly because no VC used it. In 2005 all the VCs used staid colors like maroon, navy blue, and forest green, because they were trying to appeal to LPs, not founders. The YC logo itself is an inside joke: the Viaweb logo had been a white V on a red circle, so I made the YC logo a white Y on an orange square.\\n\\n[14] YC did become a fund for a couple years starting in 2009, because it was getting so big I could no longer afford to fund it personally. But after Heroku got bought we had enough money to go back to being self-funded.\\n\\n[15] I\\'ve never liked the term \"deal flow,\" because it implies that the number of new startups at any given time is fixed. This is not only false, but it\\'s the purpose of YC to falsify it, by causing startups to be founded that would not otherwise have existed.\\n\\n[16] She reports that they were all different shapes and sizes, because there was a run on air conditioners and she had to get whatever she could, but that they were all heavier than she could carry now.\\n\\n[17] Another problem with HN was a bizarre edge case that occurs when you both write essays and run a forum. When you run a forum, you\\'re assumed to see if not every conversation, at least every conversation involving you. And when you write essays, people post highly imaginative misinterpretations of them on forums.', mimetype='text/plain', start_char_idx=69062, end_char_idx=73526, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7759472787251075), NodeWithScore(node=TextNode(id_='36d47d1e-93ef-45b2-9fc1-b37b94095d08', embedding=None, metadata={'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e78b14b2-3965-422c-8243-88726ffe0d08', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}, hash='75d96bd23dc5a771b221b0dd27e6c5d67b028f2bc39ae0e8cf53e0ded1f10145'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5ceb08e8-287a-4af7-981c-8b7590dfcba6', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}, hash='3d722f3aae2fc965e9cf7cf69f61114a40d6db5d67b671fe0d1093e4abddeb2f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ad8c3aa7-6a85-4222-9571-f2dc46b11b05', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8f2628daa10774366ce5a8f36ab12b7ba30049958abdac6ce990a34034600934')}, text='Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant.\\n\\nShe liked to paint on big, square canvases, 4 to 5 feet on a side. One day in late 1994 as I was stretching one of these monsters there was something on the radio about a famous fund manager. He wasn\\'t that much older than me, and was super rich. The thought suddenly occurred to me: why don\\'t I become rich? Then I\\'ll be able to work on whatever I want.\\n\\nMeanwhile I\\'d been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me when I visited him in Cambridge, where he was now in grad school at Harvard. It seemed to me that the web would be a big deal. I\\'d seen what graphical user interfaces had done for the popularity of microcomputers. It seemed like the web would do the same for the internet.\\n\\nIf I wanted to get rich, here was the next train leaving the station. I was right about that part. What I got wrong was the idea. I decided we should start a company to put art galleries online. I can\\'t honestly say, after reading so many Y Combinator applications, that this was the worst startup idea ever, but it was up there. Art galleries didn\\'t want to be online, and still don\\'t, not the fancy ones. That\\'s not how they sell. I wrote some software to generate web sites for galleries, and Robert wrote some to resize images and set up an http server to serve the pages. Then we tried to sign up galleries. To call this a difficult sale would be an understatement. It was difficult to give away. A few galleries let us make sites for them for free, but none paid us.\\n\\nThen some online stores started to appear, and I realized that except for the order buttons they were identical to the sites we\\'d been generating for galleries. This impressive-sounding thing called an \"internet storefront\" was something we already knew how to build.\\n\\nSo in the summer of 1995, after I submitted the camera-ready copy of ANSI Common Lisp to the publishers, we started trying to write software to build online stores. At first this was going to be normal desktop software, which in those days meant Windows software. That was an alarming prospect, because neither of us knew how to write Windows software or wanted to learn. We lived in the Unix world. But we decided we\\'d at least try writing a prototype store builder on Unix. Robert wrote a shopping cart, and I wrote a new site generator for stores  in Lisp, of course.\\n\\nWe were working out of Robert\\'s apartment in Cambridge. His roommate was away for big chunks of time, during which I got to sleep in his room. For some reason there was no bed frame or sheets, just a mattress on the floor. One morning as I was lying on this mattress I had an idea that made me sit up like a capital L. What if we ran the software on the server, and let users control it by clicking on links? Then we\\'d never have to write anything to run on users\\' computers. We could generate the sites on the same server we\\'d serve them from. Users wouldn\\'t need anything more than a browser.\\n\\nThis kind of software, known as a web app, is common now, but at the time it wasn\\'t clear that it was even possible. To find out, we decided to try making a version of our store builder that you could control through the browser. A couple days later, on August 12, we had one that worked. The UI was horrible, but it proved you could build a whole store through the browser, without any client software or typing anything into the command line on the server.\\n\\nNow we felt like we were really onto something. I had visions of a whole new generation of software working this way. You wouldn\\'t need versions, or ports, or any of that crap. At Interleaf there had been a whole group called Release Engineering that seemed to be at least as big as the group that actually wrote the software. Now you could just update the software right on the server.\\n\\nWe started a new company we called Viaweb, after the fact that our software worked via the web, and we got $10,000 in seed funding from Idelle\\'s husband Julian. In return for that and doing the initial legal work and giving us business advice, we gave him 10% of the company. Ten years later this deal became the model for Y Combinator\\'s.', mimetype='text/plain', start_char_idx=24245, end_char_idx=28530, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7668305225802323)], metadata={'4c7065ab-2ed6-424a-b8ac-0fc77cf62f7a': {'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}, '36d47d1e-93ef-45b2-9fc1-b37b94095d08': {'file_path': '/Users/juan/Documents/proyectos_propios/jobs/teamstation/RAG/data/paul_graham/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-09-26', 'last_modified_date': '2024-09-26'}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"query string\")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.vector_stores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbedding\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageContext\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_stores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaiss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaissVectorStore\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set your OpenAI API key in the environment\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.vector_stores'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Set your OpenAI API key in the environment\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Create an OpenAIEmbedding instance with your API key\n",
    "openai_embedding = OpenAIEmbedding(api_key=api_key)\n",
    "\n",
    "# Define the dimensionality of your embeddings (e.g., 768 for some models)\n",
    "embedding_dimension = 1536  # Adjust this based on the model you are using\n",
    "\n",
    "# Initialize the FAISS index using IndexFlatL2\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dimension)\n",
    "\n",
    "# Example data: Replace this with your actual documents\n",
    "documents = [\"This is a sample document.\", \"This is another document.\"]\n",
    "\n",
    "# Get embeddings for all documents using the OpenAI embedding instance\n",
    "embeddings = np.array(openai_embedding.embed_documents(documents)).astype('float32')\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "# Create a FaissVectorStore with the initialized FAISS index\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "# Create a StorageContext with the FaissVectorStore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create a VectorStoreIndex from documents using the StorageContext\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# Retrieve all nodes from the index\n",
    "all_nodes = index.docstore.docs.values()\n",
    "\n",
    "# Print out all node IDs and their embeddings\n",
    "print(\"All Nodes in the Index:\")\n",
    "for node in all_nodes:\n",
    "    print(f\"Node ID: {node.id_}, Embedding: {node.embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
