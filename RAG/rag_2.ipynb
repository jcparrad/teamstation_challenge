{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref  https://docs.llamaindex.ai/en/stable/examples/retrievers/recurisve_retriever_nodes_braintrust/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-embeddings-together\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-openai\n",
    "# %pip install llama-index-readers-file\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-readers-file\n",
    "# %pip install -U llama_hub llama_index braintrust autoevals pypdf pillow transformers torch torchvision\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-core\n",
    "# !pip install llama-index-llms-openai\n",
    "# !pip install llama-index-postprocessor-rankllm-rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --no-binary :all: nmslib\n",
    "#!CFLAGS=\"-mavx -DWARN(a)=(a)\" pip install nmslib\n",
    "#CFLAGS=\"-mavx -DWARN(a)=(a)\" pip install nmslib\n",
    "#CFLAGS=\"-mavx -DWARN(a)=(a)\" pip install --use-pep517 nmslib\n",
    "# CFLAGS=\"-mavx -DWARN(a)=(a)\" pip install 'rectools[nmslib]' \n",
    "\n",
    "# THIS IS WHAT WORKED: https://github.com/nmslib/nmslib/issues/538\n",
    "# !pip install --upgrade pybind11        # 2.10.1 or higher  (latest as of today: 2.11.1)\n",
    "# !pip install --verbose  'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank-llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q cohere llama-index-postprocessor-cohere-rerank\n",
    "# %pip install llama-index-postprocessor-rankllm-rerank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Add the parent directory containing both 'EDA' and 'RAG' to sys.path\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))  # Adjust this based on your project structure\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "from src_rag.document_loader import DocumentLoader\n",
    "from src_rag.index_manager import IndexManager\n",
    "from RAG.src_rag.text_data_handler import TextDataHandler\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import json\n",
    "from llama_index.core import Document\n",
    "\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node-23<br>**Similarity:** 0.3156899788929284<br>**Text:** How do healthcare providers educate patients about the benefits and risks of Keytruda? Healthcare providers offer educational materials, detailed consultations, and regular follow-up to discuss treatment progress and side effects.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node-21<br>**Similarity:** 0.2795438054011855<br>**Text:** How is awareness being raised for Keytruda as a treatment option? Awareness is being raised through patient advocacy groups, clinical trial publications, and healthcare provider education programs.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node-20<br>**Similarity:** 0.27798582154825885<br>**Text:** Are there patient support programs available for Keytruda users? Yes, there are several patient support programs that provide financial assistance and counseling services.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node-22<br>**Similarity:** 0.2750527397610798<br>**Text:** Is there a community support group for patients undergoing treatment with Keytruda? Yes, several online and local support groups are available for sharing experiences and advice.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node-3<br>**Similarity:** 0.2684294247400273<br>**Text:** What should patients report immediately while on Keytruda treatment? Patients should report any new or worsening symptoms such as cough, chest pain, or changes in vision immediately.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = \"../data/processed_data/df_cluster.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Check if the 'text' column exists\n",
    "if 'text' not in df.columns:\n",
    "    raise ValueError(\"The 'text' column is not present in the provided Excel file.\")\n",
    "\n",
    "\n",
    "# Create Document objects from the 'text' column\n",
    "docs = [Document(text=row['text']) for _, row in df.iterrows()]\n",
    "\n",
    "# get API key and create embeddings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "base_nodes = node_parser.get_nodes_from_documents(docs)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(base_nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "\n",
    "#embed_model = resolve_embed_model(\"local:BAAI/bge-small-en\")\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "base_index = VectorStoreIndex(base_nodes, embed_model=embed_model)\n",
    "\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "\n",
    "retrievals = base_retriever.retrieve(\n",
    "    \"Can you tell me about the key concepts for keykundra\"\n",
    ")\n",
    "\n",
    "for n in retrievals:\n",
    "    display_source_node(n, source_length=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COn reranker\n",
    "ref  https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/rankLLM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = base_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.postprocessor.rankllm_rerank import RankLLMRerank\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_reranker = True\n",
    "\n",
    "# query_str = \"Can you tell me about the key concepts for keykundra\"\n",
    "# query_bundle = QueryBundle(query_str)\n",
    "# # configure retriever\n",
    "# retriever = VectorIndexRetriever(\n",
    "#     index=index,\n",
    "#     similarity_top_k=5,\n",
    "# )\n",
    "# retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "# retrieved_nodes.reverse()\n",
    "\n",
    "# if with_reranker:\n",
    "#         # configure reranker\n",
    "#         reranker = RankLLMRerank(\n",
    "#             model=model, top_n=reranker_top_n, window_size=window_size\n",
    "#         )\n",
    "#         retrieved_nodes = reranker.postprocess_nodes(\n",
    "#             retrieved_nodes, query_bundle\n",
    "#         )\n",
    "\n",
    "#         # clear cache, rank_zephyr uses 16GB of GPU VRAM\n",
    "#         del reranker\n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip show rank-llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-core\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-postprocessor-rankllm-rerank\n",
    "# %pip install rank-llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.postprocessor.rankllm_rerank import RankLLMRerank\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def get_retrieved_nodes(\n",
    "    query_str,\n",
    "    vector_top_k=10,\n",
    "    reranker_top_n=3,\n",
    "    with_reranker=False,\n",
    "    model=\"rank_zephyr\",\n",
    "    window_size=None,\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    # configure retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=vector_top_k,\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "    retrieved_nodes.reverse()\n",
    "\n",
    "    if with_reranker:\n",
    "        # configure reranker\n",
    "        reranker = RankLLMRerank(\n",
    "            model=model, top_n=reranker_top_n#, window_size=window_size\n",
    "        )\n",
    "        retrieved_nodes = reranker.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle\n",
    "        )\n",
    "\n",
    "        # clear cache, rank_zephyr uses 16GB of GPU VRAM\n",
    "        del reranker\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"\")))\n",
    "\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(pd.DataFrame(result_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rank_llm.result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mget_retrieved_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you tell me about the key concepts for keykundra\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreranker_top_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_reranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mget_retrieved_nodes\u001b[0;34m(query_str, vector_top_k, reranker_top_n, with_reranker, model, window_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m retrieved_nodes\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_reranker:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# configure reranker\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     reranker \u001b[38;5;241m=\u001b[39m \u001b[43mRankLLMRerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreranker_top_n\u001b[49m\u001b[38;5;66;43;03m#, window_size=window_size\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     retrieved_nodes \u001b[38;5;241m=\u001b[39m reranker\u001b[38;5;241m.\u001b[39mpostprocess_nodes(\n\u001b[1;32m     33\u001b[0m         retrieved_nodes, query_bundle\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# clear cache, rank_zephyr uses 16GB of GPU VRAM\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/proyectos_propios/jobs/teamstation/.venv/lib/python3.11/site-packages/llama_index/postprocessor/rankllm_rerank/base.py:47\u001b[0m, in \u001b[0;36mRankLLMRerank.__init__\u001b[0;34m(self, model, top_n, with_retrieval, step_size, gpt_model)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model type. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvicuna\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzephyr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_llm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresult\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Result\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     50\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     51\u001b[0m     top_n\u001b[38;5;241m=\u001b[39mtop_n,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     gpt_model\u001b[38;5;241m=\u001b[39mgpt_model,\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m Result\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rank_llm.result'"
     ]
    }
   ],
   "source": [
    "new_nodes = get_retrieved_nodes(\n",
    "    \"Can you tell me about the key concepts for keykundra\",\n",
    "    vector_top_k=10,\n",
    "    reranker_top_n=3,\n",
    "    with_reranker=True,\n",
    "    model=\"gpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_llm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'window_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mget_retrieved_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhich date did Paul Gauguin arrive in Arles?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreranker_top_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_reranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank_zephyr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m visualize_retrieved_nodes(new_nodes)\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mget_retrieved_nodes\u001b[0;34m(query_str, vector_top_k, reranker_top_n, with_reranker, model, window_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m retrieved_nodes\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_reranker:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# configure reranker\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     reranker \u001b[38;5;241m=\u001b[39m \u001b[43mRankLLMRerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreranker_top_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     retrieved_nodes \u001b[38;5;241m=\u001b[39m reranker\u001b[38;5;241m.\u001b[39mpostprocess_nodes(\n\u001b[1;32m     33\u001b[0m         retrieved_nodes, query_bundle\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# clear cache, rank_zephyr uses 16GB of GPU VRAM\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'window_size'"
     ]
    }
   ],
   "source": [
    "new_nodes = get_retrieved_nodes(\n",
    "    \"Which date did Paul Gauguin arrive in Arles?\",\n",
    "    vector_top_k=50,\n",
    "    reranker_top_n=3,\n",
    "    with_reranker=True,\n",
    "    model=\"rank_zephyr\",\n",
    "    window_size=15,\n",
    ")\n",
    "\n",
    "visualize_retrieved_nodes(new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
